{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fef46c42220>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scripts import NN\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Train autoencoder\n",
    "autoencoder = NN.NeuralNetwork(8,3,8)\n",
    "identity_matrix = np.array([[1,0,0,0,0,0,0,0],\n",
    "                           [0,1,0,0,0,0,0,0],\n",
    "                           [0,0,1,0,0,0,0,0],\n",
    "                           [0,0,0,1,0,0,0,0],\n",
    "                           [0,0,0,0,1,0,0,0],\n",
    "                           [0,0,0,0,0,1,0,0],\n",
    "                           [0,0,0,0,0,0,1,0],\n",
    "                           [0,0,0,0,0,0,0,1]])\n",
    "autoencoder.train(identity_matrix, identity_matrix, batch_size=4, num_epochs=10000, learning_rate=2)\n",
    "autoencoder.epoch_losses\n",
    "plt.plot(autoencoder.epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99758326, 0.0005506 , 0.00060786, 0.00099432, 0.00058822,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.00006573, 0.99697292, 0.        , 0.        , 0.00024709,\n",
       "        0.00040556, 0.        , 0.00044117],\n",
       "       [0.0002548 , 0.        , 0.99054983, 0.00585186, 0.00000165,\n",
       "        0.00000001, 0.00309074, 0.        ],\n",
       "       [0.00134197, 0.00000023, 0.00424207, 0.99042177, 0.        ,\n",
       "        0.00506657, 0.        , 0.        ],\n",
       "       [0.00176942, 0.00141839, 0.00000156, 0.        , 0.99835849,\n",
       "        0.        , 0.00100245, 0.00000016],\n",
       "       [0.00000018, 0.0007285 , 0.        , 0.00437635, 0.        ,\n",
       "        0.99184552, 0.        , 0.00231863],\n",
       "       [0.00000004, 0.00000006, 0.0045079 , 0.        , 0.00107816,\n",
       "        0.        , 0.99343359, 0.00363395],\n",
       "       [0.        , 0.00168363, 0.        , 0.        , 0.00000263,\n",
       "        0.00442327, 0.00188964, 0.99431053]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get outputs for trained autoencoder\n",
    "\n",
    "outputs = autoencoder.predict(identity_matrix)\n",
    "np.set_printoptions(suppress=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "2a. To encode DNA sequences, I implemented one hot encoding. Each of the four DNA nucleotides is mapped to a unique 4-bit array. For example, \"A\" maps to [1,0,0,0]. This yields a 2D array (dimensions: len(seq)x4) where each row is the 4-bit array representing each nucleotide. Then the array is flattened to yield a 1D array where every 4 values represents one nucleotide from the original sequence. A 17-mer would be encoded as a 68-length binary array. I think this representation will not affect the network's predictions because there is no implied similarity or dissimilarity between any of the four encodings. For example, an alternate encoding would be to map A, T, C, G to 1, 2, 3, 4, respectively. This would inadvertantly imply more similarity between A and T than between A and G (1 is closer to 1 than to 4).\n",
    "\n",
    "3a. For my training regime, I decided to use all of the examples from the positive training data, and to subsample examples from the negative training data. I made a function, sample_subseqs(), that can take in the negative training data, and do random sampling to get a specified number of subsequences of specified length. For example, it could get 137 subsequences of length 17. The function can also take in a np.array of sequences that should be excluded from the returned subsequences, so I can input in the positive examples to that argument to ensure that none of the positive examples coincidentally show up in the negative examples too. For my training regime, to prevent the negative training data from overwhelming the positive training data, I decided to build to do a 1:1 ratio of positive:negative data. Therefore, I will use my sample_subseqs() function to generate 137 17mers from the negative data, and ensure that none of those 17mers also show up in the positive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scripts import NN\n",
    "\n",
    "# Read in sequences, subsample negative data, and do encoding\n",
    "pos_seqs = NN.read_fasta(\"data/rap1-lieb-positives.txt\")\n",
    "neg_seqs_all = NN.read_fasta(\"data/yeast-upstream-1k-negative.fa\")\n",
    "neg_seqs = NN.sample_subseqs(neg_seqs_all, 17, 137, pos_seqs)\n",
    "pos = NN.encode(pos_seqs)\n",
    "neg = NN.encode(neg_seqs)\n",
    "pos_out = np.ones(shape=(137, 1))\n",
    "neg_out = np.zeros(shape=(137, 1))\n",
    "\n",
    "# Combine & shuffle data for training\n",
    "all_in_out = np.column_stack((np.row_stack((pos, neg)), np.row_stack((pos_out, neg_out))))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(all_in_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training\n",
    "part2_nn = NN.NeuralNetwork(68, 50, 1)\n",
    "part2_nn.train(all_in_out[:,0:68], all_in_out[:,68:69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000861, 0.0000086 , 0.00000859])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Visualize losses\n",
    "plt.plot(part2_nn.epoch_losses)\n",
    "\n",
    "# Get the last few losses\n",
    "np.set_printoptions(suppress=True)\n",
    "part2_nn.epoch_losses[-4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate this initial model just using all the training data\n",
    "part2_preds = part2_nn.predict(all_in_out[:,0:68])\n",
    "part2_auc = roc_auc_score(all_in_out[:,68:69], part2_preds)\n",
    "part2_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sequence:\n",
      "ACATCCGTGCACCTCCG\n",
      "Input for positive sequence:\n",
      "[1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
      "Output for positive sequence:\n",
      "[[0.9999979]]\n",
      "Negative sequence:\n",
      "CACTACCACAGAGGGGC\n",
      "Input for negative sequence:\n",
      "[0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0]\n",
      "Output for negative sequence:\n",
      "[[0.00000001]]\n"
     ]
    }
   ],
   "source": [
    "# 4a\n",
    "\n",
    "# Positive example (the 0th sequence from the positive data file)\n",
    "print(\"Positive sequence:\")\n",
    "print(pos_seqs[0])\n",
    "print(\"Input for positive sequence:\")\n",
    "print(pos[0])\n",
    "print(\"Output for positive sequence:\")\n",
    "pos_example_out = part2_nn.predict(pos[0])\n",
    "print(pos_example_out)\n",
    "\n",
    "# Negative example (the 0th sequence from the negative data file)\n",
    "print(\"Negative sequence:\")\n",
    "print(neg_seqs[0])\n",
    "print(\"Input for negative sequence:\")\n",
    "print(neg[0])\n",
    "print(\"Output for negative sequence:\")\n",
    "neg_example_out = part2_nn.predict(neg[0])\n",
    "np.set_printoptions(suppress=True)\n",
    "print(neg_example_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b. For my network architecture, I used a 3-layer network with an input layer size 68, hidden layer size 50, and output layer size 1. The hyperparameters used were the defaults that I had set in my code. They are: batch size 10, 1000 epochs, learning rate 0.1, and loss threshold 0.00000000001. My training data had a random shuffled order of positive and negative data, to ensure that batches were unlikely to be entirely positive data or entirely negative data. The results of my training looked promising. My ROC AUC was 1.0, which is very good. This should be taken with a grain of salt though because the model was trained on all the data, and then tested on the very same data. In terms of minimizing error, the model seemed to perform well. As illustrated in the plot above, the loss decreased over the course of the training and the average loss for the final epoch was 0.00000859.\n",
    "\n",
    "4c. My stop criteria for convergence of my learned parameters was: the average loss per epoch starts increasing and the average loss per epoch has dropped below a user-specified loss threshold. I decided on the former criterion because if loss increases, it indicates that the model is probably no longer improving. It is possible that the model is in a local minimum, which is why I also included the latter criterion. If the loss is below the threshold, even if the model is possibly in a local minimum, any further improvements are relatively inconsequential.\n",
    "\n",
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 10 folds\n",
    "folds = np.array_split(all_in_out, 10)\n",
    "\n",
    "# Function to do 10fold cross validation\n",
    "# h = num nodes in hidden layer, b = batch size, e = num epochs, l = learning rate\n",
    "def cv(h, b, e, l):\n",
    "    # List of aucs for each fold\n",
    "    aucs = []\n",
    "    # Go through each fold\n",
    "    for i in range(10):\n",
    "        # Make neural network\n",
    "        nn = NN.NeuralNetwork(68, h, 1)\n",
    "        # Split into training and validation data\n",
    "        training_data = np.row_stack(folds[0:i] + folds[i:10])\n",
    "        validation_data = folds[i]\n",
    "        # Do training\n",
    "        nn.train(training_data[:,0:68], training_data[:,68:69], b, e, l)\n",
    "        # Do classification\n",
    "        preds = nn.predict(validation_data[:,0:68])\n",
    "        # Do evaluation\n",
    "        auc = roc_auc_score(validation_data[:,68:69], preds)\n",
    "        aucs.append(auc)\n",
    "    # Return aucs\n",
    "    return aucs\n",
    "\n",
    "# Do cv\n",
    "part3_aucs = cv(50, 10, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for each fold from 10-fold cross validation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"AUC for each fold from 10-fold cross validation:\")\n",
    "part3_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5a. I can use k-fold cross validation to determine my model's performance by splitting my data into folds, holding one out as validation data, and then using the rest as training data. Then I can compute a ROC AUC for the results from the validaion set for each fold.\n",
    "\n",
    "5b. I selected the value of k to be 10. For kfold cross validation, common values of k are 5, 10, and 20. I picked 10 because it's a reasonable middle ground and would result in folds containing around 27 sequences each. This seemed like a big enough fold size that the fold AUC would be informative, but also small enough that it would sufficiently capture any variance or stochasticity in the dataset.\n",
    "\n",
    "5c. The relevant metric of performance for each fold that I chose was ROC AUC because that's a good performance metric for a classification problem, which this problem is. My model performed well. All of the AUCs were 1.0, which is the highest possible value for an AUC.\n",
    "\n",
    "# Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, I've decided to do a few rounds of grid search to optimize the hyperparameters. I'm trying out different combinations of: the number of nodes in the hidden layer, the batch size, the number of epochs, and the learning rate. To assess performance, I use AUC from 10fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list containing the results of the first grid search\n",
    "# where the 0th column is the number of nodes in the hidden layer,\n",
    "# the 1st column is the batch size, the 2nd column is the number\n",
    "# of epochs, the 3rd column is the learning rate, and the 4th\n",
    "# column is the average fold auc for the CV of that combo of\n",
    "# hyperparameters\n",
    "gs1 = []\n",
    "\n",
    "# Do the grid search\n",
    "for h in [20,50]: # size of hidden layer\n",
    "    for b in [10,20]: # batch size\n",
    "        for e in [10,100]: # number of epochs\n",
    "            for l in [0.1,1,2]: # learning rate\n",
    "                aucs = cv(h, b, e, l)\n",
    "                gs1.append((h, b, e, l, np.mean(aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer size, batch size, num epochs, learning rate, avg fold auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(20, 10, 10, 0.1, 1.0),\n",
       " (20, 10, 10, 1, 1.0),\n",
       " (20, 10, 10, 2, 1.0),\n",
       " (20, 10, 100, 0.1, 1.0),\n",
       " (20, 10, 100, 1, 1.0),\n",
       " (20, 10, 100, 2, 1.0),\n",
       " (20, 20, 10, 0.1, 1.0),\n",
       " (20, 20, 10, 1, 1.0),\n",
       " (20, 20, 10, 2, 0.36832188469793514),\n",
       " (20, 20, 100, 0.1, 1.0),\n",
       " (20, 20, 100, 1, 1.0),\n",
       " (20, 20, 100, 2, 0.36832188469793514),\n",
       " (50, 10, 10, 0.1, 1.0),\n",
       " (50, 10, 10, 1, 0.15326195279976793),\n",
       " (50, 10, 10, 2, 0.09606597184328276),\n",
       " (50, 10, 100, 0.1, 1.0),\n",
       " (50, 10, 100, 1, 0.15326195279976793),\n",
       " (50, 10, 100, 2, 0.09606597184328276),\n",
       " (50, 20, 10, 0.1, 1.0),\n",
       " (50, 20, 10, 1, 0.08376781248629989),\n",
       " (50, 20, 10, 2, 0.18900560224089638),\n",
       " (50, 20, 100, 0.1, 1.0),\n",
       " (50, 20, 100, 1, 0.08376781248629989),\n",
       " (50, 20, 100, 2, 0.18900560224089638)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"hidden layer size, batch size, num epochs, learning rate, avg fold auc\")\n",
    "gs1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results, the lower learning rate (0.1) performed better than the higher learning rates (1 and 2). For the next round of grid search, I'm going to keep the learning rate at 0.1, and focus more on the size of the hidden layer and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = []\n",
    "for h in [20,40,60]: # size of hidden layer\n",
    "    for b in [10,20,30]: # batch size\n",
    "        for e in [10,100]: # number of epochs\n",
    "            aucs = cv(h, b, e, 0.1)\n",
    "            gs2.append((h, b, e, np.mean(aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer size, batch size, num epochs, avg fold auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(20, 10, 10, 1.0),\n",
       " (20, 10, 100, 1.0),\n",
       " (20, 20, 10, 1.0),\n",
       " (20, 20, 100, 1.0),\n",
       " (20, 30, 10, 1.0),\n",
       " (20, 30, 100, 1.0),\n",
       " (40, 10, 10, 1.0),\n",
       " (40, 10, 100, 1.0),\n",
       " (40, 20, 10, 1.0),\n",
       " (40, 20, 100, 1.0),\n",
       " (40, 30, 10, 1.0),\n",
       " (40, 30, 100, 1.0),\n",
       " (60, 10, 10, 1.0),\n",
       " (60, 10, 100, 1.0),\n",
       " (60, 20, 10, 1.0),\n",
       " (60, 20, 100, 1.0),\n",
       " (60, 30, 10, 1.0),\n",
       " (60, 30, 100, 1.0)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"hidden layer size, batch size, num epochs, avg fold auc\")\n",
    "gs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this round, all of the AUCs were 1.0, so I'm going to try \"up the stakes\" for the next round to get a better sense of which hyperparameter values work better. To \"up the stakes\", I will lower the number of epochs to 5. I'm less concerned with tuning that parameter than the other ones, because I assume that generally speaking, a higher number of epochs will yield better performance than a lower number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs3 = []\n",
    "for h in [20,40,60]: # size of hidden layer\n",
    "    for b in [10,20,30]: # batch size\n",
    "        aucs = cv(h, b, 5, 0.1)\n",
    "        gs3.append((h, b, np.mean(aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer size, batch size, avg fold auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(20, 10, 1.0),\n",
       " (20, 20, 1.0),\n",
       " (20, 30, 1.0),\n",
       " (40, 10, 1.0),\n",
       " (40, 20, 1.0),\n",
       " (40, 30, 1.0),\n",
       " (60, 10, 1.0),\n",
       " (60, 20, 1.0),\n",
       " (60, 30, 1.0)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"hidden layer size, batch size, avg fold auc\")\n",
    "gs3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, challenge accepted. Let's try 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer size, batch size, avg fold auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(20, 10, 1.0),\n",
       " (20, 20, 1.0),\n",
       " (20, 30, 1.0),\n",
       " (40, 10, 1.0),\n",
       " (40, 20, 0.9994444444444444),\n",
       " (40, 30, 1.0),\n",
       " (60, 10, 1.0),\n",
       " (60, 20, 1.0),\n",
       " (60, 30, 1.0)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4 = []\n",
    "for h in [20,40,60]: # size of hidden layer\n",
    "    for b in [10,20,30]: # batch size\n",
    "        aucs = cv(h, b, 1, 0.1)\n",
    "        gs4.append((h, b, np.mean(aucs)))\n",
    "print(\"hidden layer size, batch size, avg fold auc\")\n",
    "gs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is kind of a challenging result to interpret. The only non-1 AUC was the combo with the middle values for hidden layer size & batch size, so it's hard to tell if the model performed better with increasing or decreasing those values. I will try turn up the heat even more by increasing the learning rate back to 1 and see what happens with the hidden layer size & batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer size, batch size, avg fold auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(20, 10, 1.0),\n",
       " (20, 20, 0.9963426909225228),\n",
       " (20, 30, 0.3156079016688261),\n",
       " (40, 10, 0.7555400715064581),\n",
       " (40, 20, 0.7253677088551038),\n",
       " (40, 30, 0.7370906940864923),\n",
       " (60, 10, 0.2733460971801308),\n",
       " (60, 20, 0.2851477339187423),\n",
       " (60, 30, 0.2852316521014)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs5 = []\n",
    "for h in [20,40,60]: # size of hidden layer\n",
    "    for b in [10,20,30]: # batch size\n",
    "        aucs = cv(h, b, 1, 1)\n",
    "        gs5.append((h, b, np.mean(aucs)))\n",
    "print(\"hidden layer size, batch size, avg fold auc\")\n",
    "gs5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! A trend! It seems that smaller hidden layer sizes (20) are better than bigger hidden layer sizes (60). It seems that smaller batch sizes work better when the hidden layer size is smaller and larger batch sizes work better when the hidden layer size is larger. Moving forward, I think I will do a hidden layer size of 20 and batch size of 10. Lastly, I will tune the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num epochs, avg fold auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(100, 1.0), (1000, 1.0)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs6 = []\n",
    "for e in [100,1000]: # size of hidden layer\n",
    "    aucs = cv(20, 10, e, 1)\n",
    "    gs6.append((e, np.mean(aucs)))\n",
    "print(\"num epochs, avg fold auc\")\n",
    "gs6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, both had an AUC of 1. I think more epochs is a safer bet than fewer epochs. Overall, the set of learning parameters that works the best is: hidden layer size of 20, batch size of 10, learning rate of 0.1, and number of epochs of 1000. Sample outputs from my system for optimization are above (the 6 rounds of grid search). The effects of altering my system depended on the hyperparameter. More epochs yielded better results. This is unsurprising and I think we observe these effects because it gives the model \"more chances\" to learn the data and get it right. The model seemed to work best for low values for: hidden layer size, batch size, and learning rate. This result was somewhat surprising for hidden layer size, as I had assumed a bigger hidden layer would be better. I think that's mostly because I read some random rule of thumb on the internet that the hidden layer should be roughly 2/3 the size of the input layer. But I guess I shouldn't always believe everything I read on the internet. Also it's possible the results of my optimization are wrong. Due to time/memory constraints, I couldn't do a huge grid search of lots of hyperparameter values at once, so I did the process in 6 rounds, making changes here and there. It's possible that the ideal hyperparameter combo is a combo that I didn't cover with my grid search. It seems reasonable that a smaller batch size was better. This would let the model slowly learn the data bit by bit, instead of in enormous chunks. Also it makes sense that a smaller learning rate was better. This would ensure the model didn't update weights to quickly or dramatically, and my number of epochs is pretty big (1000), so that's plenty of time to learn. Other parameters that may affect performance are the choice of activation function, although I'm under the impression that sigmoid is a reasonable/standard activation function for these types of problems. Also, perhaps a different number of hidden layers would impact the performance, but my implementation doesn't allow for that.\n",
    "\n",
    "# Part 5\n",
    "\n",
    "6a. For my final model, I will use a 68x20x1 network with batch size 10, learning rate 0.1, and 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b\n",
    "\n",
    "# Train network on all the data\n",
    "part6_nn = NN.NeuralNetwork(68, 20, 1)\n",
    "part6_nn.train(all_in_out[:,0:68], all_in_out[:,68:69], 10, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00110013],\n",
       "       [0.7176767 ],\n",
       "       [0.999956  ],\n",
       "       ...,\n",
       "       [0.00678338],\n",
       "       [0.00754665],\n",
       "       [0.1749296 ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run on test data\n",
    "test_seqs = NN.read_fasta(\"data/rap1-lieb-test.txt\")\n",
    "test = NN.encode(test_seqs)\n",
    "test_preds = part6_nn.predict(test)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['AAAAAAACGCAACTAAT', '0.0011001348624881265'],\n",
       "       ['AAAAACACACATCTGGC', '0.7176767007066459'],\n",
       "       ['AAAACCAAACACCTGAA', '0.9999559968237722'],\n",
       "       ...,\n",
       "       ['TTTCTCATACACCTTTA', '0.0067833792550472125'],\n",
       "       ['TTTTCCAAGCATTTGTA', '0.007546651164360559'],\n",
       "       ['TTTTCCATACCTTCCTT', '0.17492960103400187']], dtype='<U32')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format output for outfile\n",
    "test_seqs_preds = np.column_stack((test_seqs, test_preds))\n",
    "test_seqs_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('<U32') and format specifier ('%.18e\t%.18e')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1446\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not numpy.str_",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-73509d88ccdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write to outfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"part5-results.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seqs_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1447\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m                     raise TypeError(\"Mismatch between array dtype ('%s') and \"\n\u001b[0m\u001b[1;32m   1450\u001b[0m                                     \u001b[0;34m\"format specifier ('%s')\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m                                     % (str(X.dtype), format))\n",
      "\u001b[0;31mTypeError\u001b[0m: Mismatch between array dtype ('<U32') and format specifier ('%.18e\t%.18e')"
     ]
    }
   ],
   "source": [
    "# Write to outfile\n",
    "np.savetxt(\"part5-results.tsv\", test_seqs_preds, fmt='%s', delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
